\setchapterstyle{kao}
\setchapterpreamble[u]{\margintoc}

\chapter{Search for Tau Neutrino Induced Heavy Neutral Lepton Events}
\labch{analysis}

This chapter describes the search for HNL events using \SI{10}{years} of IceCube DeepCore data. The expected number of HNL events in the data sample depends on the mass of the additional heavy state, $m_4$, and the mixing element $|U_{\alpha4}^2|$, with $\alpha=e,\mu,\tau$, between the SM flavors and the new mass state. As discussed in \refsec{hnl_theory}, this work focuses on the mixing to the tau sector, $|U_{\tau4}^2|$, which has the weakest constraints to date. Since the mass itself influences the production and decay kinematics of the event and the accessible decay modes, individual mass samples were produced as described in \refsec{model_specific_simulation}. The mass influences the decay length and energy distributions, while the mixing both changes the overall expected rate of the HNL events and the shape in energy and length. We perform three independent searches for each mass sample, where the mixing is measured in each of the fits.

The analysis itself is performed by comparing the MC distributions to the observed data. To produce the  physically expected distributions, the MC events are then weighted given a specific choice of physics  and nuisance parameters. By binning them and calculating a loss function comparing the bin expectations to the data, the physics and nuisance parameters that best correspond to the observed data are estimated by minimizing this loss function. This is referred to as the \textit{fit} to data.


\section{Final Level Sample} \labsec{analysis_samples}

The final level simulation sample of this analysis consists of the neutrino and muon MC introduced in \refsec{sm_event_generation} and one of the three HNL samples explained in \refsec{model_specific_simulation}, while the data are the events measured in \SI{10}{years} of IceCube DeepCore data taking. All simulation and the data are processed through the full event selection chain described in \refsec{processing_chain} and \refsec{reconstruction} leading to the final level sample. As described in \refsec{analysis_cuts}, event triggers consisting purely of random coincidences induced by noise in the DOMs have been reduced to a negligible rate, and will not be discussed further.

To get the neutrino expectation, the MC events are weighted according to their generation weight introduced in \refsec{neutrino_generation}, multiplied by the total lifetime, and the expected neutrino flux. For the correct expectation at the detector, the events have to be weighted by the oscillation probability, depending on their energy and their distance traveled from the atmosphere to the detector. The oscillation probabilities are calculated using a \textsc{PYTHON} implementation of the calculations from \sidecite{prob3}, which use the matter profile of the Earth following the \textit{Preliminary Reference Earth Model (PREM)} \sidecite{PREM} as input. Apart from the energy and the distance, the two relevant parameters defining the oscillation probabilities are the atmospheric neutrino oscillation parameters $\theta_{23}$ and $\Delta m^{2}_{31}$. Since the HNL events originate from the tau neutrinos that were produced as muon neutrinos in the atmosphere and then oscillated into $\nu_\tau$, this weighting is also applied in addition to the specific weighting scheme for the HNL events described in \refsec{hnl_weighting_scheme}, which itself is defined by the mixing $|U_{\tau4}^2|$ and the mass $m_4$.


\subsection{Expected Rates/Events}

The rates and the expected number of events for the SM background are shown in \reftab{background_final_level_expectation} with around 175000 total events expected in the \SI{10}{years}. Only data marked as good is used for the analysis, where \textit{good} refers to measurement time with the correct physics run configuration and without other known issues. The resulting good detector livetime in this data taking period was \SI{9.28}{years}. The rates are calculated by summing the weights of all events in the final level sample, while the uncertainties are calculated by taking the square root of the sum of the weights squared. The expected number of events is calculated by multiplying the rate with the livetime. The individual fractions show that this sample is neutrino dominated where the majority of events are $\nu_\mu$-CC events.

\begin{table}[h]
    \begin{tabular}{ ccrr }
    \hline\hline
    \textbf{Type} & \textbf{Rate [\si{\milli\hertz}]} & \textbf{Events (\SI{9.28}{years})} & \textbf{Fraction [\si{\percent}]} \\ 
    \hline\hline
    $\nu_\mu^\rm{CC}$   & 0.3531 & 103321 $\pm$ 113 & 58.9 \\
    $\nu_e^\rm{CC}$     & 0.1418 & 41490 $\pm$ 69 & 23.7 \\
    $\nu^\rm{NC}$       & 0.0666 & 19491 $\pm$ 47 & 11.1 \\
    $\nu_\tau^\rm{CC}$  & 0.0345 & 10094 $\pm$ 22 & 5.8 \\
    $\mu_\rm{atm}$      & 0.0032 & 936 $\pm$ 15 & 0.5 \\
    \hline
    total               & 0.5992 & 175332 $\pm$ 143 & 100.0  \\
    \hline
    \end{tabular}
\caption[Final level background event/rate expectation]{Final level rates and event expectation of the SM background particle types.}
\labtab{background_final_level_expectation}
\end{table}

\reftab{signal_final_level_expectation} shows the rates and expected number of events for the HNL signal simulation. The expectation depends on the mass and the mixing and shown here are two example mixings for all the three masses that are being tested in this work. A mixing of $0.0$ would result in no HNL events at all. It can already be seen that for the smaller mixing of $|U_{\tau4}|^2=10^{-3}$ the expected number of events is very low, while at the larger mixing of $|U_{\tau4}|^2=10^{-1}$ the number is comparable to the amount of atmospheric muons in the background sample. 

\begin{table}[h]
    \begin{tabular}{ lcc }
    \hline\hline

    \textbf{HNL mass} & \textbf{Rate [\si{\micro\hertz}]} & \textbf{Events (in \SI{9.28}{years})} \\

    \hline\hline
    \textbf{$|U_{\tau4}|^2=10^{-1}$} & & \\ 
    \hline
    \SI{0.3}{\gev} & 3.3 & 975 $\pm$ 2 \\
    \SI{0.6}{\gev} & 3.1 & 895 $\pm$ 2 \\
    \SI{1.0}{\gev} & 2.5 & 731 $\pm$ 2 \\
    \hline
    \textbf{$|U_{\tau4}|^2=10^{-3}$} & & \\ 
    \hline
    \SI{0.3}{\gev} & 0.006 & 1.67 $\pm$ 0.01 \\
    \SI{0.6}{\gev} & 0.022 & 6.44 $\pm$ 0.01 \\
    \SI{1.0}{\gev} & 0.025 & 7.27 $\pm$ 0.01 \\
    \hline
    \end{tabular}
\caption[Final level signal event/rate expectation]{Final level rates and event expectations of the HNL signal for all three masses and two example mixing values.}
\labtab{signal_final_level_expectation}
\end{table}


\subsection{Analysis Binning}

\begin{table}[h]
    \small
    \begin{tabular}{ llll }
    \hline\hline    
    \textbf{Variable} & \textbf{$N_\rm{bins}$} & \textbf{Edges} & \textbf{Spacing} \\     
    \hline\hline    
    $P_\nu$ & 3 & [0.00, 0.25, 0.55, 1.00] & linear \\
    $E$ & 12 & [5.00, 100.00] & logarithmic \\
    $\cos(\theta)$ & 8 & [-1.00, 0.04] & linear \\    
    \hline
\end{tabular}
\caption[Analysis binning]{Three-dimensional binning used in the analysis. All variables are from the FLERCNN reconstruction explained in \refsec{reconstruction}.}
\labtab{analysis_binning}
\end{table}

An identical binning to the analysis performed in \sidecite{flercnn_analysis_result} is used. In total, there are three bins in PID (cascade-like, mixed, and track-like), 12 bins in reconstructed energy, and 8 bins in cosine of the reconstructed zenith angle as specified in \reftab{analysis_binning}.

\begin{figure}[h]
    \includegraphics{figures/results/3d_histograms/labeled_s_to_sqrt_b_1.0_GeV_combined_U_tau4_sq_0.1000_total.png}
    \caption[Three-dimensional signal over square root of background expectation]{Signal over square root of background expectation in \SI{9.28}{years} for the \SI{1.0}{\gev} mass sample at a mixing of $0.1$, while all other parameters are at their nominal values.}
    \labfig{s_to_sqrt_b_1.0_GeV_0.1_mixing}
\end{figure}

Extending the binning towards lower energies or increasing the number of bins in energy or cosine of the zenith angle did not improve the HNL sensitivities significantly, because the dominant signal region is already covered with a sufficiently fine binning to observe the shape and magnitude of the HNL events on top of the SM background. This can be seen in the middle panel of \reffig{s_to_sqrt_b_1.0_GeV_0.1_mixing}, which shows the expected signal events divided by the square root of the expected background events for every bin used in the analysis. The signal expectation is using the \SI{1.0}{\gev} mass sample at a reference mixing of $0.1$, with the corresponding three-dimensional histogram shown in \reffig{signal_1.0_GeV_0.1_mixing}. Both the nominal background expectation used to calculate the signal to square root of background ratio and the detector data can be seen in \reffig{background_and_data_3d_hist}.

\begin{figure}[h]
    \includegraphics{figures/results/3d_histograms/all_background.png}
    \includegraphics{figures/results/3d_histograms/all_data.png}
    \caption[Three-dimensional background expectation and observed data]{Background expectation in \SI{9.28}{years} for all other parameters are at their nominal values (top) and observed data (bottom).}
    \labfig{background_and_data_3d_hist}
\end{figure}

Some low energy bins in the cascade-like region have very low MC expectations (<1 event) and are therefore not taken into account in the analysis, to prevent unwanted behavior in the fit. Those are shown in dark green in the three-dimensional histograms, and both background and data histograms show a strong decrease of events towards low energies in the cascade-like bin. This background expectation is not necessarily supposed to agree with the data, because this is the distributions assuming nominal parameter values, before performing the fit to find the parameters that describe the data best. All parameters used in the analysis are discussed in \refsec{analysis_parameters}, and post-fit data to MC comparisons are shown in \refsec{data_mc_agreement}.


\section{Statistical Analysis} \labsec{analysis_principle}

\subsection{Test Statistic}

The measurements are performed by comparing the weighted MC to the data. Through variation of the nuisance and physics parameters that govern the weights, the best matching set of parameters can be found, by optimizing a fit metric. The comparison is done using a modified $\chi^2$, defined as
\begin{equation}
    \chi^2_{\mathrm{mod}} = 
    \sum_{i \in \mathrm{bins}}^{}\frac{(N^{\rm{exp}}_i - N^{\mathrm{obs}}_i)^2}
    {N^{\rm{exp}}_i + (\sigma^{\mathrm{\nu}}_i)^2 + (\sigma^{\mathrm{\mu}}_i)^2 + (\sigma^{\mathrm{HNL}}_i)^2}
     + \sum_{j \in \mathrm{syst}}^{}\frac{(s_j - \hat{s_j})^2}{\sigma^2_{s_j}}
    \;,
    \labeq{mod-chi2-hnl}
\end{equation}
as the fit metric. It is designed such that taking the difference between a free fit and a fit with fixed parameters based on a chosen hypothesis, $\Delta\chi^2_{\mathrm{mod}}$, can directly be used as a \textit{test statistic (TS)} for hypothesis testing, due to its asymptotic behavior. The total even expectation is $N^{\rm{exp}}_i = N^{\mathrm{\nu}}_i + N^{\mathrm{\mu}}_i + N^{\mathrm{HNL}}_i$, where $N^{\mathrm{\nu}}_i$, $N^{\mathrm{\mu}}_i$, and $N^{\mathrm{HNL}}_i$ are the expected number of events in bin $i$ from neutrinos, atmospheric muons, and HNLs, while $N^{\mathrm{obs}}_i$ is the observed number of events in the bin. The expected number of events from each particle type is calculated by summing the weights of all events in the bin $N^{\mathrm{type}}_i = \sum_i^\rm{type}\omega_i$, with the statistical uncertainty being $(\sigma^{\mathrm{type}}_i)^2 = \sum_i^\rm{type}\omega_i^2$. The additional term in \refeq{mod-chi2-hnl} is included to apply a penalty term for prior knowledge of the systematic uncertainties of the parameters where they are known. $s_j$ are the systematic parameters that are varied in the fit, while $\hat{s_j}$ are their nominal values and $\sigma_{s_j}$ are the known uncertainties.


\subsection{Physics Parameters} \labsec{analysis_parameters}

The variable physics parameter in this analysis is the mixing between the HNL and the SM $\tau$ sector, \ut4. It is varied continuously in the range of [\SI{0.0}, \SI{1.0}] by applying the weighting scheme described in \refsec{hnl_weighting_scheme}. The fit is initialized at an off-nominal value of 0.1. The other physics parameter, the mass $m_4$ of the HNL, is implicitly fixed to one of the three discrete masses to be tested, by using the corresponding sample of the HNL simulation described in \refsec{model_specific_simulation}.


\subsection{Nuisance Parameters} \labsec{systematic_uncertainties}

There are multiple sources of systematic uncertainties related to the event generation and detector simulation explained in \refch{signal_simulation}. All uncertainties considered in this work need to be implemented with parameters that can be varied continuously so that a simultaneous fit of the physics and systematic parameters can be performed. Where possible, a correct model of the effect is used, but in many cases the variations are captured by effective parameters. Uncertainties that solely scale the total event rate are not included individually, since the analysis only uses the relative distribution of events and a single scaling parameter $N_{\nu}$ is used to scale the total neutrino rate instead.


\subsubsection{Atmospheric Flux Uncertainties}
\begin{figure}[h]
    \centering 
    \includegraphics[width=0.65\textwidth]{figures/simulation_and_processing/flux/honda_flux_spl.png}
    \caption[South pole atmospheric neutrino flux]{Atmospheric neutriono flux computed at the South Pole. Shown are the neutrino and antineutrino flux for $\nu_e$ and $\nu_\mu$. Taken from \cite{PhysRevD.92.023004_Honda_Flux}.}
    \labfig{honda_flux_spl}
\end{figure}

The flux of atmospheric neutrinos is influenced by multiple factors, the spectrum and composition of primary CRs, the atmospheric conditions, and the hadronic interaction model used to describe the air showers development. Uncertainties of the neutrino flux are therefore dictated by the uncertainties on these components, where the variations in atmospheric conditions were found to have negligible effect \sidecite{OVS_PRD}. The baseline neutrino flux used in this thesis is taken from \sidecite{PhysRevD.92.023004_Honda_Flux}. \reffig{honda_flux_spl} shows the flux for neutrinos and antineutrinos, computed at the South Pole.

\paragraph{Cosmic ray flux:}

The selected sample of atmospheric neutrinos lies around energies of up to \SI{100}{\gev}. The initial primary particles in the CR flux can have 100 times larger energies and therefore the CR flux between \SI{10}{\gev} and \SI{10}{\tera\electronvolt} is important, which dominantly consists of hydrogen and helium nuclei \sidecite{cosmic_ray_composition_and_GSF}. The uncertainty in this CR flux component can be described as a power law correction \sidecite{PhysRevD.74.094009, PhysRevD.95.023012}
\begin{equation}
    \Phi^\prime_\nu = \Phi_\nu \left( \frac{E}{E^\star} \right)^{\Delta\gamma}
    \;,
    \labeq{power_law_flux_uncertainty}
\end{equation}
where $E^\star$ is the pivot energy and $\Delta\gamma$ is the correction to the power law exponent. This modification propagates into the neutrino flux, which is therefore corrected in the same way. $E^\star$ was chosen to be \SI{24}{\gev} as to minimize the dependence of the overall flux scale on $\Delta\gamma$ \sidecite{OVS_PRD}.


\paragraph{Hadronic interaction model:}

Neutrinos are produced from the decaying hadrons (dominantly pions and kaons) in CR air shower, spanning a large parameter space that is sparsely evaluated by experimental data. To include uncertainties based on energy, direction, and neutrino flavor, the \textsc{MCEq} package \cite{mceq} is used to compute the distribution of atmospheric leptons and to estimate the impact of varying their contributions. The calculations result in the change in flux $\rm{d}\Phi_\rm{l}/\rm{d}B$ for a variation $\rm{d}B$ of some parameter $B$. Scaling this variation by some value $b$, the modified total flux, s is then given by
\begin{equation}
    \Phi^\prime_\rm{l} = \Phi_\rm{l} + \left( b \cdot \frac{\rm{d}\Phi_\rm{l}}{\rm{d}B} \right)
    \;.
    \labeq{mceq_flux_variations}
\end{equation}
Matching the work in \sidecite{Barr:2006it}, the parameter space is divided in regions of the primary energy, $E_i$, and the energy fraction of the secondary meson, $x_\rm{lab}$, with varying uncertainties, derived from fixed target experiment data. The Sibyll2.3c \sidecite{sibyll2.3d} hadronic interaction model and the GSF CR flux \sidecite{cosmic_ray_composition_and_GSF} were used to calculate the related flux changes\sidenote{The choice of flux and hadronic interaction model have negligible impact on the variations.} for the different regions in $E_i$ and $x_\rm{lab}$, resulting in 17 variables, encoding the possible changes. \reffig{bar_blocks} shows the selected regions of the parameter space and the names given to the uncertainties. At the energies relevant for this work, the flux is dominantly affected by the pion uncertainties. The variational term in \refeq{mceq_flux_variations} is applied for each of these parameters and the total variation is the sum of all individual variations.

\begin{figure}[h]
    \centering 
    \includegraphics{figures/simulation_and_processing/flux/barr_blocks.png}
        \caption[Hadronic model flux uncertainty regions in hadron phase space]{Flux uncertainty regions of the hadronic interaction model in the phase space of the primary energy $E_i$ and the energy fraction of the secondary meson $x_\rm{lab}$. Taken from \cite{Barr:2006it}.}
    \labfig{bar_blocks}
\end{figure}


\subsubsection{Cross-Section Uncertainties} \labsec{cross_section_uncertainties}

The uncertainties related to the cross-sections are split into low and high energy components, since there is no coherent model to explain both regimes. Below \SI{20}{\gev}, \textit{charged current resonance production (CCRES)} and \textit{charged current quasi elastic scattering (CCQE)} interactions with the nucleons as a whole are important, while above \SI{20}{\gev} DIS interactions are the dominant processes. Three parameters are included to account for all relevant cross-section uncertainties.

At low energies two parameters are included to account for uncertainties in form factors of CCQE and CCRES events. These uncertainties are due to uncertainties in the \textit{axial mass} $M_\rm{A}$, which enters the form factor as in
\begin{equation}
    F(Q^2) \sim \frac{1}{(1 - (\frac{Q}{M_\rm{A}})^2)^2}
    \;,
\end{equation}
where $Q^2$ is the momentum transfer squared. The axial mass can be determined experimentally and to include uncertainties on the values of $M_\rm{A}^\rm{CCQE}$ and $M_\rm{A}^\rm{CCRES}$, the cross-sections are computed for each event, where the form factors are calculated varying the axial mass by $\pm 20\% (1\sigma)$/$\pm 40\% (1\sigma)$ around the nominal value. This is an approximation of the recommended uncertainties by the GENIE collaboration, which are $-15\%$, $+25\%$ for $M_\rm{A}^\rm{CCQE}$ and $\pm 20\%$ for $M_\rm{A}^\rm{CCRES}$ \cite{genie}. To apply a continuous uncertainty variation of the axial mass in a fit, the total cross-section is fit with a quadratic function to interpolate between the cross-sections computed with the different axial masses.

Even though the DIS interactions can be calculated very precisely, there are still uncertainties in the input PDF, describing the probability of finding a specific parton (quark) with a specific momentum fraction $x$ inside a nucleon. To account for differences between the used method and more sophisticated methods using newer PDFs seen at high energies, an uncertainty parameter is introduced. The parameter is based on the discrepancy between the cross-sections computed with GENIE and the ones computed with CSMS \sidecite{csms} above \SI{100}{\gev}. The included parameter scales the cross-section from the GENIE values to the CSMS values, which are considered more accurate above \SI{100}{\gev}. The scaling is done as a function of energy and inelasticity and to guarantee continuity, the scaling is extrapolated linearly below \SI{100}{\gev}\sidenote{Multiple functional extrapolations were tested, but the choice was found to have negligible impact.}. The parameter is designed such that a value of 0.0 corresponds to the GENIE cross-sections and a value of 1.0 gives an approximation of the CSMS cross-sections. A comparison of the total cross-sections GENIE (scaled/unscaled) with the data is shown in \reffig{dis_systematic_effect}.

\begin{figure*}
    \centering 
    \includegraphics[width=0.49\linewidth]{figures/simulation_and_processing/cross_sections/NuMu_CC_iso_comp_to_data__upd_style.pdf}
    \includegraphics[width=0.49\linewidth]{figures/simulation_and_processing/cross_sections/NuMu_Bar_CC_iso_comp_to_data__upd_style.pdf}
    
    \caption[Inclusive total neutrino-nucleon cross-sections]{Inclusive total neutrino-nucleon cross-sections on an isoscalar target (black) for neutrinos (left) and antineutrinos (right) calculated with GENIE, comparing to measurements from NOMAD \cite{xsec_data_nomad}, NUTEV \cite{xsec_data_nutev}, and CCFR \cite{xsec_data_ccfr}. The scaled GENIE cross-section (orange) is also shown. Taken from \cite{OVS_PRD}.}
    \labfig{dis_systematic_effect}
\end{figure*}


\subsubsection{Muon Uncertainties}

The muon fraction in the final level selection (see \refsec{analysis_cuts}) is below \SI{1}{\percent}, therefore additional muon systematic uncertainties apart from the spectral index are not implemented, but rather a total muon scaling parameter is added. This total scale is somewhat degenerate with the DOM efficiency, since an increased DOM efficiency leads to better muon rejection. Both the total muon scaling and the muon spectral index have a very small impact on the analysis as will be shown in \refsec{analysis_systematics}.


\subsubsection{Detector Calibration Uncertainties} \labsec{detector_uncertainties}

The detection process of neutrinos in IceCube has several sources of uncertainties, where the effects of the properties of the ice itself and the optical efficiency of the DOMs are dominant for this analysis. None of these uncertainties can be described by an analytic expression, so instead their effects are estimated using MC simulation. This is done by producing additional simulation samples at discrete values of those parameters. The five relevant uncertainty parameters are the absolute efficiency of the DOMs, a global scaling of ice scattering and absorption lengths, and variations of the relative angular acceptance due to hole ice variations in two parameters. To perform the fit, continuous variations with respect to these parameters will be derived with a method explained in \refsec{ultrasurfaces}.


\paragraph{Relative DOM efficiency:}

As was already mentioned in \refsec{ice_and_DOMs}, the absolute efficiency of the DOMs, $\epsilon_\rm{DOM}$ is calibrated using minimum ionizing muons from air showers, due to the lack of a calibrated light source in the detector. Using the muons as a steady, controlled source of light, the efficiency can be estimated by comparing simulated muon data sets with varied DOM response to the measured data. Since the uncertainties found in multiple iterations of this study \sidecite{JFeintzeig_phd, domeff_nick} are at the order of \SI{10}{\percent}, this systematic is highly relevant and is included in the analysis.


\paragraph{Ice scattering and absorption:}

Absorption and scattering length are the most important properties that govern the propagation of photons through the ice. The simulation principle and how the depth dependent absorption and scattering coefficients are used was already explained in \refsec{photon_propagation}. To account for uncertainties on this model of the ice coefficients, a global scaling for each of the two parameters (global absoprtion, global scatteriong) is applied.

\begin{figure}[h]
    \centering
    \input{figures/simulation_and_processing/hole_ice/hole_ice_p0_p1.tex}
    \caption[Hole ice angular acceptance modification]{Relative angular acceptance modification due to hole ice. Shown is the current baseline model, the variations from changing $p_0$ and $p_1$, and a laboratory measurement. Modified from \cite{ATrettin_phd}.}
    \labfig{hole_ice_variations}
\end{figure}
\paragraph{Hole ice angular acceptance:}

Due to bubble formation in the re-freezing process of the boreholes, the hole ice seems to be less transparent in the center of the columns \sidecite{rongen_drilling_epj}. This effectively decreases the chance of photons hitting the DOMs directly from below, which can be described as an additional angular modification of the DOM acceptance. The modification is parameterized by a two-dimensional, normalized\sidenote{The hole ice angular acceptance modification is normalized so that it does not affect the total charge.} function, where the two dominant of the parameters ($p_0, p_1$), dictating its form, are enough to describe all past and the current hole ice models from both \textit{in-situ} and laboratory measurements. \reffig{hole_ice_variations} shows the acceptance modification as a function of the incident photon angle $\cos(\eta)$. The current baseline model, the variations achieved through modifying $p_0$ and $p_1$, and a laboratory measurement can be seen.


\paragraph{Ice Model:}

The ice model used in IceCube is continuously improved, and the recent models incorporate the birefringent polycriystalline microstructure \sidecite{bfr_ice_tc-18-75-2024} into the ice properties. To account for the uncertainty, due to this un-modeled effect in the ice model used for the simulation production, an additional simulation sample is produced using the newer version of the ice model, that incorporates the \textit{birefringence (BFR)} effect. 


\paragraph{Treatment of Detector Systematic Uncertainties:} \labsec{ultrasurfaces}

Since the variations related to the detector calibration uncertainties introduced in \refsec{detector_uncertainties} are estimated by simulating MC at discrete values of the systematic parameters, a method to derive continuous variations is needed to perform the fit. The method applied here was initially introduced in \sidecite{Fischer_2023} and first used in the low energy sterile neutrino search in \sidecite{ATrettin_phd} (section 7.4.3). Using a \textit{likelihood-free inference} technique, re-weighting factors are found for every event in the nominal MC sample, given a specific choice of detector systematic parameters. These factors quantify how much more or less likely the event would be for the corresponding change in detector response from the nominal parameters. Without going into the details of the method, which were already exhaustively discussed in \cite{Fischer_2023} and \cite{ATrettin_phd}, the performance is assessed here for the HNL signal simulation. In order to do so, the weights are applied to the nominal MC samples, choosing the detector systematic values used to produce the discrete samples and the resulting event expectations are compared to the expectations from the individual, discrete MC samples. The bin counts are compared by calculating the pull defined as
\begin{equation}
    p = \frac{N_{\rm{reweighted}}-N_{{\rm sys}}}{\sqrt{\sigma^2_\rm{reweighted}+\sigma^2_{\rm sys}}}
    \;,
    \labeq{bin_wise_pull}
\end{equation}
where $N$ are the bin-wise event expectations and $\sigma$ are their MC uncertainty. For the SM BG simulation, the performance was already investigated in \sidecite{ELohfink_phd} (section 7.4.4, appendix B5) and the re-weighted nominal MC was shown to be in agreement with the discrete systematic sets at a sufficient level. \reffig{hnl_ultrasurfaces_3d_pulls_selection0} shows the bin-wise pulls for the \SI{1.0}{\gev} HNL mass sample at a mixing of $0.1$ for a selection of the discrete systematic samples, where the DOM efficiency and the ice absorption was varied by $\pm10\%$. As expected, the pull distributions follow a standard normal distribution, without strong clustering or any systematic deviations. A similar performance is found for the additional systematic variations and the detailed figures can be found in \refsec{ultrasurfaces_appendix}.

\begin{figure*}[h]
    \includegraphics[width=0.9\linewidth]{figures/results/utlrasurfaces/oscNext_leptoninjector_1.0_GeV_knn_probs_neighbors_500_weighted_nfiles_extended_holeice_corrected_grads_poly_2_weighted_reference_weight_0.0100_thesis_style_subset_0-5.png}
    \caption[Detector systematic uncertainty treatment bin-wise pulls example sets]{Three-dimensional pulls and set-wise pull distributions between the nominal set and the specific systematic sets, after the nominal set was re-weighted to the corresponding systematic parameter value. Set 0001 and 0004 have the DOM efficiency varied by $\pm10\%$, while set 0504 and 0505 have the ice absorption varied by $\pm10\%$.}
    \labfig{hnl_ultrasurfaces_3d_pulls_selection0}
\end{figure*}


\subsubsection{Free Parameters} \labsec{analysis_systematics}

\begin{marginfigure}
    \includegraphics{figures/results/checks/08_systematics_impact_test_v3_results.pdf}
	\caption[Nuisance parameter mis-modeling impact ranking]{Mis-modeling impact ranking of the systematic parameters. The mis-modeling is calculated as the fit metric difference between a fit with the parameter fixed at its nominal value and a fit with the parameter pulled up by $+1\sigma$. The test was performed using Asimov data of the \SI{1.0}{\gev} mass sample at a reference mixing of $0.1$.}
    \labfig{systematic_impact_test}
\end{marginfigure}

To decide which systematic uncertainties should be included in the fit, we test the potential impact they have on the TS if they are neglected. The test is performed by creating pseudo-data sets from the MC by choosing the nominal nuisance parameters and specific physics parameters, without adding any statistical or systematic fluctuations to it. These so-called .\textit{Asimov}\sidenote{A pseudo-data set without statistical fluctuations is called Asimov data set.} are made with the BG simulation and the HNL simulation of the \SI{1.0}{\gev} mass sample at a mixing value of $0.1$, which is chosen as a benchmark physics parameter, but the explicit choice does not have a significant impact on the test. The systematic parameter of interest is set to a value above its nominal expectation, either pulled up by $+1\sigma$ or by an educated estimate for parameters without a well-defined uncertainty. A fit is performed fixing the systematic parameter of interest and leaving all additional parameters free. The resulting TS is the fit metric difference between this fit and a fit with all parameters free, which would result in a fit metric of 0.0 for this Asimov test. This difference is called mis-modeling significance and parameters below a significance of \SI{0.1}{\sigma} are fixed. The test is performed in an iterative manner until the final set of free parameters is found.

\reffig{systematic_impact_test} shows the resulting significances of one of these tests. The parameters tested are the systematic parameters introduced in \refsec{systematic_uncertainties} and the atmospheric oscillation parameters mentioned in \refsec{analysis_samples}. In the final selection of free parameters the Barr $h_{\pi^+}$ parameter was also left free, to sufficiently cover the relevant energy production range of the Pions, as can be seen in \reffig{bar_blocks}, where both for kaons and pions the uncertainties are included for primary energies above \SI{30}{\gev} and $x_\rm{lab}>0.1$. Additionally, the ice absorption is still kept free, despite showing a small significance, which is done because the ice parameters are not well constrained and are known to have a large impact, which might be concealed in this idealized test, due to correlations with the other parameters. In this test, the effect of correlations is challenging to consider, because only the impact of one parameter is tested at a time, using the overall mis-modeling significance as a measure. The mis-modeling could be reduced by a correlated parameter capturing the effect of the parameter of interest. For this reason a very conservative threshold of \SI{0.1}{\sigma} is chosen and some parameters below the threshold are still left free in the fit.

All nuisance parameters that are left free in the fit are summarized in \reftab{free_parameters}, showing their nominal values, the allowed fit ranges, and their Gaussian prior, if applicable. The scaling parameter $N_{\nu}$ is included to account for the overall normalization of the neutrino rate, and it has the identical effect on the SM neutrino events and the BSM HNL events, because they both originate from the same neutrino flux. Despite being known to $\sim$\SI{5}{\percent} in this energy range \sidecite{PhysRevD.92.023004_Honda_Flux}, there is no prior applied to this parameter, because the fit itself is able to constrain it well, which can be seen by the large impact it shows in \reffig{systematic_impact_test}. Concerning the atmospheric neutrino flux, the CR power law flux correction factor $\Delta \gamma_\nu$ introduced in \refsec{systematic_uncertainties} is included with nominal value of 0.0 which corresponds to the baseline flux model. A slightly conservative prior of 0.1 is applied to the parameter, while latest measurements show an uncertainty of 0.05 \sidecite{PhysRevD.95.023012}. The Barr parameters are contrained by a Gaussian prior, taken from \sidecite{Barr:2006it}. All the detector systematic uncertainties discussed in \refsec{detector_uncertainties} are included in the fit. The DOM efficiency $\epsilon_{\rm{DOM}}$ is constrained by a Gaussian prior with a width of $0.1$, which is a conservative estimate based on the studies of the optical efficiency using minimum ionizing muons from \sidecite{JFeintzeig_phd, domeff_nick}. The two atmospheric neutrino oscillation parameters $\theta_{23}$ and $\Delta m^{2}_{31}$ are also included in the fit with nominal values of \SI{47.5}{\degree} and \SI{2.48e-3}{\electronvolt^2} \sidecite{flercnn_analysis_result}, respectively. Since they govern the shape and the strength of the tau neutrino flux, by defining the oscillation from $\nu_\mu$ to $\nu_\tau$, they are also relevant for the HNL signal shape.

\begin{table}
    \begin{tabular}{ llll }
    \hline\hline
    \textbf{Parameter} & \textbf{Nominal} & \textbf{Range} & \textbf{Prior} \\
    \hline\hline
    $\theta_{23} [\si{\degree}]$ & 47.5047  & [0.0, 90.0] & - \\
    $\Delta m^{2}_{31} [\si{\electronvolt^2}]$ & 0.002475 & [0.001, 0.004] & - \\
    \hline
    $N_{\nu}$ & 1.0 & [0.1, 2.0] & - \\
    $\Delta \gamma_\nu$ & 0.0 & [-0.5, 0.5] & 0.1 \\
    $\rm{Barr} \, h_{\pi^+}$ & 0.0 & [-0.75, 0.75] & 0.15 \\
    $\rm{Barr} \, i_{\pi^+}$ & 0.0 & [-3.05, 3.05] & 0.61 \\
    $\rm{Barr} \, y_{K^+}$ & 0.0 & [-1.5, 1.5] & 0.3 \\
    \hline
    $\rm{DIS}$ & 0.0 & [-0.5, 1.5] & 1.0 \\
    $M_\rm{A,QE}$ & 0.0 & [-2.0, 2.0] & 1.0 \\
    $M_\rm{A,res}$ & 0.0 & [-2.0, 2.0] & 1.0\\
    \hline
    $\epsilon_{\rm{DOM}}$ & 1.0 & [0.8, 1.2] & 0.1 \\
    $\rm{hole \, ice} \, p_0$ & 0.101569 & [-0.6, 0.5] & - \\
    $\rm{hole \, ice} \, p_1$ & -0.049344  & [-0.2, 0.2] & - \\
    $\rm{ice \, absorption}$ & 1.0 & [0.85, 1.15] & - \\
    $\rm{ice \, scattering}$ & 1.05 & [0.9, 1.2] & - \\
    $N_\rm{bfr}$ & 0.0 & [-0.2, 1.2] & - \\
    \hline
    \end{tabular}
\caption[Nuisance parameter nominal values and fit ranges]{Systematic uncertainty parameters that are left free to float in the fit. Their allowed fit ranges are shown with the nominal value and the Gaussian prior width if applicable.}
\labtab{free_parameters}
\end{table}


\subsection{Low Energy Analysis Framework} \labsec{analysis_framework}

The analysis is performed using the \textsc{PISA} \sidecite{pisa_paper} \cite{pisa_software} software framework, which was developed to perform analyses of small signals in high-statistics neutrino oscillation experiments. It is used to generate the expected event distributions from several MC samples, which can then be compared to the observed data. The expectation for each MC sample is calculated by applying physics and nuisance parameter effects in a stage-wise manner, before combining them to the final expectation.


\section{Analysis Checks}

Fitting to data is performed in a \textit{blind} manner, where the analyzer does not immediately see the fitted physics and nuisance parameter values, but first checks that a set of pre-defined \textit{goodness of fit (GOF)} criteria are fulfilled. This is done to circumvent the so-called \textit{confirmation bias} \sidecite{confirmation_bias}, where the analyzer might be tempted to construct the analysis in a way that confirms their expectation. After the GOF criteria are met to satisfaction, the fit results are unblinded and the full result can be revealed. Before these blind fits to data are performed, the robustness of the analysis method is tested using pseudo-data that is generated from the MC.


\subsection{Minimization Robustness} \labsec{asimov_inject_recover}

To find the set of parameters that best describes the data, a staged minimization routine is used. In the first stage, a fit with coarse minimizer settings is performed to find a rough estimate of the \textit{best fit point (BFP)}. In the second stage, the fit is performed again in both octants\sidenote{There is a degeneracy between the lower octant ($\theta_{23}<\SI{45}{\degree}$) and the upper octant ($\theta_{23}>\SI{45}{\degree}$), which can lead to fit metric minima (local and global) at two positions that are mirrored around \SI{45}{\degree} in $\theta_{23}$.} of $\theta_{23}$, starting from the BFP of the coarse fit. For each individual fit the \textit{MIGRAD} routine of \textsc{iminuit} \sidecite{iminuit_v2.17.0} is used to minimize the $\chi^2_{\mathrm{mod}}$ fit metric defined in \refeq{mod-chi2-hnl}. Iminuit is a fast, python compatible minimizer based on the \textsc{Minuit2} C++ library \sidecite{og_minuit}. The individual minimizer settings for both stages are shown in \reftab{minimization_settings}.

\begin{margintable}
    \small
        \begin{tabular}{ llll }
        \hline\hline
        \textbf{Fit} & \textbf{Err.} & \textbf{Prec.} & \textbf{Tol.} \\
        \hline\hline
        Coarse & 1e-1 & 1e-8 & 1e-1 \\
        Fine & 1e-5 & 1e-14 & 1e-5 \\
        \hline
        \end{tabular}
    \caption[Staged minimization routine settings]{Migrad settings for the two stages in the minimization routine. \textit{Err.} are the step size for the numerical gradient estimation, \textit{Prec.} is the precision with which the LLH is calculated, and \textit{Tol.} is the tolerance for the minimization.}
    \labtab{minimization_settings}
\end{margintable}

To test the minimization routine and to make sure it consistently recovers any physics parameters, Asimov data sets are produced  and then fit back with the full analysis chain. This type of test is called \textit{Asimov inject/recover test}. A set of mixing values between $10^{-3}$ and $10^{0}$ is injected and fit back. Without fluctuations the fit is expected to always recover the injected parameters (both physics and nuisance parameters). The fitted mixing values from the Asimov inject/recover tests are compared to the true injected values in \reffig{asimov_inject_recover} for all three mass samples. As desired, the fit is always able to recover the injected physics parameter and the nuisance parameters within the statistical uncertainty or at an insignificant fit metric difference. 

\begin{figure}[h]
    \includegraphics{figures/results/checks/asimov_tests_combined.png}
	\caption[Asimov inject/recover test]{Asimov inject/recover test results for all three mass samples. Mixing values between $10^{-3}$ and $10^{0}$ are injected and fit back with the full analysis chain. The injected parameter is always recovered within the statistical uncertainty or at an insignificant fit metric difference.}
    \labfig{asimov_inject_recover}
\end{figure}


\subsection{Goodness of Fit} \labsec{pseudo_data_ensemble}

To estimate the GOF, pseudo-data is generated from the MC by injecting the BFP parameters as true parameters and then fluctuating the expected bin counts to account for MC uncertainty and Poisson fluctuations in data. First, the expectation value of each bin is drawn from a Gaussian distribution centered at the nominal expectation value with a standard deviation corresponding to the MC uncertainty of the bin. Based on this sampled expectation value, each bin count is drawn from a Poisson distribution, independently, to get the final pseudo-data set. These pseudo-data sets are analysed with the same analysis chain as the real data, resulting in a final fit metric value for each pseudo-data set. By comparing the distribution of fit metric values from this \textit{ensemble} of pseudo-data trials to the fit metric of the fit to real data, a p-value can be calculated. The p-value is the probability of finding a value of the fit metric at least as large as the one from the data fit. \reffig{pseudo_data_ensembles} shows the distribution from the ensemble tests for for all three mass samples and the observed value from the fit, resulting in p-values of \SI{28.3}{\percent}, \SI{28.7}{\percent} and \SI{26.0}{\percent}. Based on this test, it is concluded that the fit result is compatible with the expectation from the ensemble of pseudo-data trials.

\begin{figure*}[h]
    \includegraphics[width=0.32\linewidth]{figures/results/blind_fits/blind_fit_ensemble_comparison_0.3_GeV.pdf}
    \includegraphics[width=0.32\linewidth]{figures/results/blind_fits/blind_fit_ensemble_comparison_0.6_GeV.pdf}
    \includegraphics[width=0.32\linewidth]{figures/results/blind_fits/blind_fit_ensemble_comparison_1.0_GeV.pdf}
	\caption[Pseudo-data trials fit metric distributions]{Observed fit metric (data fit) and fit metric distribution from pseudo-data ensemble generated around the best fit point. Shown are the results for all three mass samples, with the ensemble distribution on orange, the observed value in black, and the p-value in the legend.}
    \labfig{pseudo_data_ensembles}
\end{figure*}


\subsection{Data/MC Agreement} \labsec{data_mc_agreement}

\begin{figure}[h]
    \includegraphics{figures/results/checks/bfp_3d_binwise_pull_0.6_GeV.pdf}
	\caption[Best fit three-dimensional bin-wise pulls (\SI{0.6}{\gev})]{Three-dimensional bin-wise pulls between data and simulation at the best fit point of the \SI{0.6}{\gev} mass sample fit.}
    \labfig{3_d_bfp_pull_0.6_GeV}
\end{figure}

At the BFP, the agreement between the data and simulation is probed by comparing both the one-dimensional analysis distributions for PID, energy, and cosine of the zenith angle as well as the full three-dimensional distributions. \reffig{3_d_bfp_pull_0.6_GeV} shows the three-dimensional pull distribution between data and the total MC expectation for the \SI{0.6}{\gev} mass sample at the BFP. The pulls are evenly spaced and show no strong clustering. In \reffig{1_d_data_mc_bfp_1.0_GeV_energy} the reconstructed energy distributions is shown for the \SI{0.6}{\gev} mass sample. The data is compared to the total MC expectation, which is also split up into individual components for illustration. Note here that the HNL events are shown as part of the $\nu_\mathrm{NC}$, since they originate from the $\nu_\tau$-NC flux. Good agreement can be observed between the distributions, which was quantified by a reduced $\chi^2$, which is close to \SI{1.0}. The distributions of the other two analysis variables are shown in \refsec{data_mc_agreement_appendix}. 

\begin{figure}[h]
    \includegraphics{figures/results/best_fit/reco_energy_data_mc_agreement.png}
	\caption[Reconstructed energy data/MC comparison]{Data/MC comparison of the reconstructed energy for the \SI{0.6}{\gev} mass sample. The data is compared to the total MC expectation, which is also split up into individual components for illustration. The HNL events are shown as part of the $\nu_\mathrm{NC}$ component.}
    \labfig{1_d_data_mc_bfp_1.0_GeV_energy}
\end{figure}


\section{Results}


\subsection{Best Fit Nuisance Parameters}

The resulting nuisance parameter values from the fits are illustrated in \reffig{best_fit_deltas_normed}, where the differences to the nominal values are shown, normalized by the distance to the closest boundary. The results from all three fits are shown in the same plot and the fits prefer values of the same size for all three mass samples. For parameters that have a Gaussian prior, the \SI{1}{\sigma} range is also displayed. As was already confirmed during the blind fit procedure, all fitted parameters are within this range. The effective ice model parameter, $N_\rm{bfr}$, prefers a value of $\sim$\SI{0.74}, indicating that the data fits better to an ice model that includes real birefringence effects \sidecite{bfr_ice_tc-18-75-2024}. For completeness, the explicit results are listed in \reftab{best_fit_parameters}. There, the nominal values and the absolute differences to the best fit value are also presented.

\begin{figure*}[h]
    \includegraphics{figures/results/best_fit/hnl_analysis_best_fit_deltas_normed_dist_to_nominal_correct_0.6_fit_updated.png}
	\caption[Best fit nuisance parameter distances to nominal]{Best fit nuisance parameter distances to the nominal values, normalized by the distance to the closest boundary. For parameters with a Gaussian prior, the $+$\SI{1}{\sigma} range is also shown.}
    \labfig{best_fit_deltas_normed}
\end{figure*}


\subsection{Agreement with Standard Model Three-Flavor Oscillation Measurement}

{\renewcommand{\arraystretch}{1.1}
\begin{margintable}
    \footnotesize
    \begin{tabular}{ ccc }
        \hline\hline
        \textbf{$m_4$} & \textbf{$\sin^2{\theta_{23}}$} & \textbf{$\Delta m^2_{32}\,[\si{\electronvolt^2}]$} \\
        \hline\hline
        \SI{0.3}{\gev} & 0.554 & $0.0238$ \\
        \SI{0.6}{\gev} & 0.551 & $0.0238$ \\
        \SI{1.0}{\gev} & 0.553 & $0.0238$ \\
        \hline
    \end{tabular}
\caption[Best fit oscillation parameters]{Best fit oscillation parameters from the three mass sample fits. The values are compatible with the IceCube result within the uncertainties.}
\labtab{hnl_oscilation_parameters}
\end{margintable}
}

The recently performed atmospheric neutrino oscillation measurement by the IceCube collaboration resulted in a best fit point of $\sin^2{\theta_{23}} = 0.544^{+0.030}_{-0.096}$ and $\Delta m^2_{32} = 2.40^{+0.03}_{-0.06} \times 10^{-3}\si{\electronvolt^2}$ \sidecite{flercnn_analysis_result}. The result used the identical \SI{10}{years} of data at the same final level selection, assuming normal mass ordering. The differences to this analysis are the choice of fit metric and the use of the previous treatment of detector systematic uncertainties. Both the choice of the fit metric and the detector systematic uncertainty treatment should not influence the best fit values and a naive comparison of the results to the results from this work is done, to validate this is the case. The best fit values are listed in \reftab{hnl_oscilation_parameters} and are all compatible with the IceCube result within the uncertainties. Since they are statistically fully dependent, a more rigorous quantitative comparison would be more involved and is not performed here. The agreement is still interpreted as a first validation of the new detector systematics treatment. 


\subsection{Best Fit Parameters and Limits}

The fitted mixing values are
\begin{align*}
    |U_{\tau4}|^2(\SI{0.3}{\gev}) &= 0.003^{+0.084} \;, \\
    |U_{\tau4}|^2(\SI{0.6}{\gev}) &= 0.080^{+0.134} \;, \rm{and} \\
    |U_{\tau4}|^2(\SI{1.0}{\gev}) &= 0.106^{+0.132} \;,
\end{align*}
with their $+$\SI{1}{\sigma} uncertainty. All of them are compatible with the null hypothesis of \SI{0.0} mixing, although the \SI{0.6}{\gev} and \SI{1.0}{\gev} fits indicate a mixing value of \SI{0.08} and \SI{0.106}, respectively. The best fit mixing values and the corresponding upper limits at \SI{68}{\percent} and \SI{90}{\percent} \textit{confidence level (CL)} are listed in \reftab{best_fit_parameters_and_confidence_levels}, also showing the $p$-value to reject the null hypothesis. The CLs and $p$-value are estimated by assuming that \textit{Wilks' theorem} \sidecite[-0.5cm]{the_not_to_be_mentioned_theorem} holds, meaning that the TS follows a $\chi^2$ distribution with one degree of freedom.

\begin{table}[h]
    \begin{tabular}{ ccccc }
        \hline\hline
        \textbf{HNL mass} & \textbf{\ut4} & \textbf{68 \si{\percent} CL} & \textbf{90 \si{\percent} CL} & \textbf{NH $p$-value} \\    
        \hline\hline
        \SI{0.3}{\gev} & 0.003 & 0.09 & 0.19 & \SI{0.97}{} \\
        \SI{0.6}{\gev} & 0.080 & 0.21 & 0.36 & \SI{0.79}{} \\
        \SI{1.0}{\gev} & 0.106 & 0.24 & 0.40 & \SI{0.63}{} \\
        \hline
    \end{tabular}
    \caption[Best fit mixing values and confidence limits]{Best fit mixing values and the corresponding upper limits at \SI{68}{\percent} and \SI{90}{\percent} confidence level, as well as the $p$-value to reject the null hypothesis, estimated by assuming that Wilks' theorem holds.}
    \labtab{best_fit_parameters_and_confidence_levels}
\end{table}

\reffig{brazil_bands} shows the observed TS profiles as a function of \ut4 for all three fits. The TS profile is the difference in $\chi^2_{\mathrm{mod}}$ between the free fit and a fit where the mixing is fixed to a specific value. Also shown is the expected TS profile, based on 100 pseudo-data trials, produced at the BFP and then fluctuated using both Poisson and Gaussian fluctuations, to include the data and the MC uncertainty as was explained in \refsec{pseudo_data_ensemble}. The Asimov expectation and the \SI{68}{\percent} and \SI{90}{\percent} bands are shown and the observed TS profiles lie within the \SI{68}{\percent} band for all three, confirming that they are compatible with statistical fluctuations of the observed data. For the \SI{0.3}{\gev} fit, the observed contour is slightly tighter than the Asimov expectation, meaning that the observed upper limits in \ut4 are slightly stronger than expected. For the \SI{0.6}{\gev} the opposite is the case and the observed upper limit is therefore slightly weaker than expected. For the \SI{1.0}{\gev} fit, the observed upper limit is very close to the Asimov expectation in the region where the \SI{68}{\percent} and \SI{90}{\percent} CLs thresholds are crossed. The observed upper limits are also shown in \reftab{best_fit_parameters_and_confidence_levels}.


\section{Summary and Outlook}

A measurement of the mixing parameter \ut4 for HNLs with masses \SI{0.3}{\gev}, \SI{0.6}{\gev}, and \SI{1.0}{\gev} is performed through a binned, maximum likelihood fit, using ten years of IceCube DeepCore data. No significant signal of HNLs is found, and the best fit mixing values obtained are consistent with the null hypothesis of no mixing. The fits constrain the mixing parameter to \ut4$ < 0.19\;(m_4 = \SI{0.3}{\gev})$, \ut4$ < 0.36\;(m_4 = \SI{0.6}{\gev})$, and \ut4$ < 0.40\;(m_4 = \SI{1.0}{\gev})$ at \SI{90}{\percent} confidence level. Despite these limits being several orders of magnitude below the current leading limits on \ut4, this initial result serves as a proof of concept for HNL searches both in IceCube and using atmospheric neutrinos in general.

The analysis is expected improve in the future, and a few key aspects of potential future HNL searches are discussed here. This work applied the low-energy, atmospheric neutrino event selection, which was designed to reduce muons and noise and results in a neutrino dominated sample, and in \refsec{dc_reconstruction_performance} it was shown that the selection has a similar efficiency for HNLs as for neutrinos. The analysis in its current form is only possible through this strong reduction of muons and noise, but a targeted event selection, aiming for an analysis distinguishing HNLs from SM background could improve the sensitivity significantly.

In the current analysis setup, the HNL signal treated as a cascade signature, which is true for most of the events, but not all of them. Extending the binning to include dimensions aiming to discriminate between unique HNL event signatures and the SM events, could improve the sensitivity significantly. Additionally, the event sample could be extended to include HNL events that were produced outside the detector and decay inside or vice versa. This would give a more realistic estimate of the expected number of single-cascade events. The current event generator is targeted for both production and decay of the HNL inside the detector and therefore only provides a conservative estimate of the expected single-cascade event rate.

Since the atmospheric neutrino flux also contains large numbers of muon neutrinos and smaller contributions from electron neutrinos, both the mixing \ue4 and \um4 could also be probed using the same data set. This would require further development on the event generator, as additional decay modes have to be added. On top of that, the current experimental limits for these parameters are much stronger than for \ut4 and significant improvements are required to perform a competitive analysis. For any of the three flavors, additional BSM coupling processes, like the dipole coupling, could also be investigated, but would require a dedicated study to understand the potentially different event signatures.

Lastly, the future low-energy extension IceCube upgrade will provide a significant improvement in the sensitivity to low-energy events through a reduced spacing and a segmented directionality of the optical modules. This enhances the light detection and should yield a better chance to identify the unique HNL signature. The data taking is expected to begin in 2026.

\begin{figure*}[h]
    \includegraphics[width=0.59\linewidth]{figures/results/best_fit/brazil_band_with_asimov_0.3_GeV_updated_with_bfp_with_1sigma.png}
    \includegraphics[width=0.59\linewidth]{figures/results/best_fit/brazil_band_with_asimov_0.6_GeV_updated_with_bfp_with_1sigma.png}
    \includegraphics[width=0.59\linewidth]{figures/results/best_fit/brazil_band_with_asimov_1.0_GeV_updated_with_bfp_with_1sigma.png}
	\caption[Best fit point TS profiles]{Best fit point TS profiles as a function of \ut4 for the \SI{0.3}{\gev}, \SI{0.6}{\gev}, and \SI{1.0}{\gev} mass samples. Shown are the observed profiles, the Asimov expectation at the best fit point, and the \SI{68}{\percent} and \SI{90}{\percent} bands, based on 100 pseudo-data trials. Also indicated are the \SI{68}{\percent} and \SI{90}{\percent} CL levels assuming Wilks' theorem.}
    \labfig{brazil_bands}
\end{figure*}
